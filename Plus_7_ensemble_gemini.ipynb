{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AutoGluon Ensemble for California Housing\n",
                "\n",
                "This notebook uses AutoGluon to train and ensemble multiple models (GBM, LGBM, XGBoost, CatBoost, Random Forest) on the California Housing dataset.\n",
                "It includes Weighted Ensemble (Voting) and Stacking techniques."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.datasets import fetch_california_housing\n",
                "from sklearn.model_selection import train_test_split\n",
                "from autogluon.tabular import TabularDataset, TabularPredictor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset Shape: (20640, 9)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>MedInc</th>\n",
                            "      <th>HouseAge</th>\n",
                            "      <th>AveRooms</th>\n",
                            "      <th>AveBedrms</th>\n",
                            "      <th>Population</th>\n",
                            "      <th>AveOccup</th>\n",
                            "      <th>Latitude</th>\n",
                            "      <th>Longitude</th>\n",
                            "      <th>MedHouseVal</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>8.3252</td>\n",
                            "      <td>41.0</td>\n",
                            "      <td>6.984127</td>\n",
                            "      <td>1.023810</td>\n",
                            "      <td>322.0</td>\n",
                            "      <td>2.555556</td>\n",
                            "      <td>37.88</td>\n",
                            "      <td>-122.23</td>\n",
                            "      <td>4.526</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>8.3014</td>\n",
                            "      <td>21.0</td>\n",
                            "      <td>6.238137</td>\n",
                            "      <td>0.971880</td>\n",
                            "      <td>2401.0</td>\n",
                            "      <td>2.109842</td>\n",
                            "      <td>37.86</td>\n",
                            "      <td>-122.22</td>\n",
                            "      <td>3.585</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>7.2574</td>\n",
                            "      <td>52.0</td>\n",
                            "      <td>8.288136</td>\n",
                            "      <td>1.073446</td>\n",
                            "      <td>496.0</td>\n",
                            "      <td>2.802260</td>\n",
                            "      <td>37.85</td>\n",
                            "      <td>-122.24</td>\n",
                            "      <td>3.521</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>5.6431</td>\n",
                            "      <td>52.0</td>\n",
                            "      <td>5.817352</td>\n",
                            "      <td>1.073059</td>\n",
                            "      <td>558.0</td>\n",
                            "      <td>2.547945</td>\n",
                            "      <td>37.85</td>\n",
                            "      <td>-122.25</td>\n",
                            "      <td>3.413</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>3.8462</td>\n",
                            "      <td>52.0</td>\n",
                            "      <td>6.281853</td>\n",
                            "      <td>1.081081</td>\n",
                            "      <td>565.0</td>\n",
                            "      <td>2.181467</td>\n",
                            "      <td>37.85</td>\n",
                            "      <td>-122.25</td>\n",
                            "      <td>3.422</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
                            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
                            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
                            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
                            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
                            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
                            "\n",
                            "   Longitude  MedHouseVal  \n",
                            "0    -122.23        4.526  \n",
                            "1    -122.22        3.585  \n",
                            "2    -122.24        3.521  \n",
                            "3    -122.25        3.413  \n",
                            "4    -122.25        3.422  "
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load dataset\n",
                "data = fetch_california_housing()\n",
                "df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
                "df['MedHouseVal'] = data['target']\n",
                "\n",
                "print(\"Dataset Shape:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train size: (16512, 9), Test size: (4128, 9)\n"
                    ]
                }
            ],
            "source": [
                "# Split into Train and Test sets (e.g., 80% Train, 20% Test)\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Train size: {train_df.shape}, Test size: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. AutoGluon Training\n",
                "\n",
                "We will configure AutoGluon to train specific models:\n",
                "- GBM (LightGBM)\n",
                "- CAT (CatBoost)\n",
                "- XGB (XGBoost)\n",
                "- RF (Random Forest)\n",
                "\n",
                "And use `presets='best_quality'` to enable Stacking and Bagging. AutoGluon automatically creates a Weighted Ensemble."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Verbosity: 2 (Standard Logging)\n",
                        "=================== System Info ===================\n",
                        "AutoGluon Version:  1.5.0\n",
                        "Python Version:     3.11.14\n",
                        "Operating System:   Windows\n",
                        "Platform Machine:   AMD64\n",
                        "Platform Version:   10.0.26200\n",
                        "CPU Count:          32\n",
                        "Pytorch Version:    2.9.1+cpu\n",
                        "CUDA Version:       CUDA is not available\n",
                        "Memory Avail:       74.84 GB / 127.76 GB (58.6%)\n",
                        "Disk Space Avail:   852.19 GB / 1861.70 GB (45.8%)\n",
                        "===================================================\n",
                        "Presets specified: ['best_quality']\n",
                        "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
                        "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
                        "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
                        "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
                        "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
                        "2026-01-28 16:22:43,416\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
                        "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
                        "2026-01-28 16:22:52,054\tINFO worker.py:2014 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
                        "c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\site-packages\\ray\\_private\\worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
                        "  warnings.warn(\n",
                        "\t\tContext path: \"c:\\Users\\user\\github\\DataScience\\scikit-learn\\scikit-learn\\ag_models_california_housing\\ds_sub_fit\\sub_fit_ho\"\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Running DyStack sub-fit ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Beginning AutoGluon training ... Time limit = 138s\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m AutoGluon will save models to \"c:\\Users\\user\\github\\DataScience\\scikit-learn\\scikit-learn\\ag_models_california_housing\\ds_sub_fit\\sub_fit_ho\"\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Train Data Rows:    14677\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Train Data Columns: 8\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Label Column:       MedHouseVal\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Problem Type:       regression\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Preprocessing data ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Using Feature Generators to preprocess the data ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tAvailable Memory:                    75660.90 MB\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tTrain Data (Original)  Memory Usage: 0.90 MB (0.0% of available memory)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tStage 1 Generators:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tStage 2 Generators:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tStage 3 Generators:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tStage 4 Generators:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tStage 5 Generators:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.0s = Fit runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t8 features in original data used to generate 8 features in processed data.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.90 MB (0.0% of available memory)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Data preprocessing and feature engineering runtime = 0.02s ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m User-specified model hyperparameters to be fit:\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m {\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t'GBM': [{}],\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t'CAT': [{}],\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t'XGB': [{}],\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t'RF': [{}],\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m }\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 91.99s of the 138.01s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Exception in thread Thread-2 (_readerthread):\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Traceback (most recent call last):\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m   File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m     self.run()\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m   File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\threading.py\", line 982, in run\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m   File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m     buffer.append(fh.read())\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m                   ^^^^^^^^^\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m   File \"<frozen codecs>\", line 322, in decode\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 4: invalid start byte\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.03%)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(_ray_fit pid=56132)\u001b[0m [1000]\tvalid_set's l2: 0.222258\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.201\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t2.35s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.24s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: RandomForest_BAG_L1 ... Training model for up to 82.28s of the 128.31s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=32, gpus=0, mem=0.1/72.9 GB\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.2541\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t1.57s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.38s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 79.99s of the 126.02s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.53%)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(_ray_fit pid=55904)\u001b[0m [2000]\tvalid_set's l2: 0.195191\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(_ray_fit pid=61720)\u001b[0m \tRan out of time, early stopping on iteration 9443.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1884\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t64.2s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.03s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 12.22s of the 58.25s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.2058\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t1.95s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.06s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 138.02s of the 52.58s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.0/71.0 GB\n",
                        "\u001b[36m(_ray_fit pid=63120)\u001b[0m \tRan out of time, early stopping on iteration 9470.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.72, 'LightGBM_BAG_L1': 0.16, 'XGBoost_BAG_L1': 0.08, 'RandomForest_BAG_L1': 0.04}\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1868\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.03s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.0s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 52.52s of the 52.51s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.03%)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1883\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.86s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.02s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: RandomForest_BAG_L2 ... Training model for up to 47.42s of the 47.42s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=32, gpus=0, mem=0.1/71.4 GB\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1972\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t2.84s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.45s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 43.87s of the 43.86s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.54%)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1869\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t2.62s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.02s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 37.51s of the 37.51s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1902\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.75s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.02s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 138.02s of the 32.58s of remaining time.\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.0/70.5 GB\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.36, 'CatBoost_BAG_L2': 0.32, 'LightGBM_BAG_L2': 0.16, 'XGBoost_BAG_L2': 0.16}\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t-0.1847\t = Validation score   (-mean_squared_error)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.04s\t = Training   runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m \t0.0s\t = Validation runtime\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m AutoGluon training complete, total runtime = 105.52s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 4121.5 rows/s (1835 batch size)\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\github\\DataScience\\scikit-learn\\scikit-learn\\ag_models_california_housing\\ds_sub_fit\\sub_fit_ho\")\n",
                        "\u001b[36m(_dystack pid=55320)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
                        "Leaderboard on holdout data (DyStack):\n",
                        "                 model  score_holdout  score_val         eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
                        "0  WeightedEnsemble_L3      -0.203830  -0.184676  mean_squared_error        1.658525       0.777876  74.344966                 0.002554                0.000637           0.040985            3       True         10\n",
                        "1      LightGBM_BAG_L2      -0.204901  -0.188256  mean_squared_error        1.558011       0.738228  70.928612                 0.015906                0.015342           0.863825            2       True          6\n",
                        "2      CatBoost_BAG_L2      -0.205354  -0.186898  mean_squared_error        1.558568       0.740313  72.685279                 0.016462                0.017426           2.620492            2       True          8\n",
                        "3  WeightedEnsemble_L2      -0.206187  -0.186768  mean_squared_error        1.544674       0.723400  70.097613                 0.002568                0.000514           0.032826            2       True          5\n",
                        "4       XGBoost_BAG_L2      -0.207075  -0.190186  mean_squared_error        1.623603       0.744471  70.819664                 0.081497                0.021585           0.754877            2       True          9\n",
                        "5      CatBoost_BAG_L1      -0.207353  -0.188383  mean_squared_error        0.513330       0.034911  64.201397                 0.513330                0.034911          64.201397            1       True          3\n",
                        "6  RandomForest_BAG_L2      -0.213417  -0.197215  mean_squared_error        1.870568       1.175099  72.901735                 0.328462                0.452213           2.836948            2       True          7\n",
                        "7      LightGBM_BAG_L1      -0.214274  -0.200990  mean_squared_error        0.431824       0.244387   2.348439                 0.431824                0.244387           2.348439            1       True          1\n",
                        "8       XGBoost_BAG_L1      -0.218062  -0.205793  mean_squared_error        0.301764       0.064039   1.945532                 0.301764                0.064039           1.945532            1       True          4\n",
                        "9  RandomForest_BAG_L1      -0.267702  -0.254102  mean_squared_error        0.295187       0.379549   1.569419                 0.295187                0.379549           1.569419            1       True          2\n",
                        "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
                        "\t123s\t = DyStack   runtime |\t477s\t = Remaining runtime\n",
                        "Starting main fit with num_stack_levels=1.\n",
                        "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
                        "Beginning AutoGluon training ... Time limit = 477s\n",
                        "AutoGluon will save models to \"c:\\Users\\user\\github\\DataScience\\scikit-learn\\scikit-learn\\ag_models_california_housing\"\n",
                        "Train Data Rows:    16512\n",
                        "Train Data Columns: 8\n",
                        "Label Column:       MedHouseVal\n",
                        "Problem Type:       regression\n",
                        "Preprocessing data ...\n",
                        "Using Feature Generators to preprocess the data ...\n",
                        "Fitting AutoMLPipelineFeatureGenerator...\n",
                        "\tAvailable Memory:                    74344.80 MB\n",
                        "\tTrain Data (Original)  Memory Usage: 1.01 MB (0.0% of available memory)\n",
                        "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
                        "\tStage 1 Generators:\n",
                        "\t\tFitting AsTypeFeatureGenerator...\n",
                        "\tStage 2 Generators:\n",
                        "\t\tFitting FillNaFeatureGenerator...\n",
                        "\tStage 3 Generators:\n",
                        "\t\tFitting IdentityFeatureGenerator...\n",
                        "\tStage 4 Generators:\n",
                        "\t\tFitting DropUniqueFeatureGenerator...\n",
                        "\tStage 5 Generators:\n",
                        "\t\tFitting DropDuplicatesFeatureGenerator...\n",
                        "\tTypes of features in original data (raw dtype, special dtypes):\n",
                        "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
                        "\tTypes of features in processed data (raw dtype, special dtypes):\n",
                        "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
                        "\t0.0s = Fit runtime\n",
                        "\t8 features in original data used to generate 8 features in processed data.\n",
                        "\tTrain Data (Processed) Memory Usage: 1.01 MB (0.0% of available memory)\n",
                        "Data preprocessing and feature engineering runtime = 0.04s ...\n",
                        "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
                        "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
                        "\tTo change this, specify the eval_metric parameter of Predictor()\n",
                        "User-specified model hyperparameters to be fit:\n",
                        "{\n",
                        "\t'GBM': [{}],\n",
                        "\t'CAT': [{}],\n",
                        "\t'XGB': [{}],\n",
                        "\t'RF': [{}],\n",
                        "}\n",
                        "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
                        "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
                        "Fitting model: LightGBM_BAG_L1 ... Training model for up to 317.82s of the 476.86s of remaining time.\n",
                        "Exception in thread Thread-4 (_readerthread):\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
                        "    self.run()\n",
                        "  File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\threading.py\", line 982, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"c:\\Users\\user\\miniconda3\\envs\\DS\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
                        "    buffer.append(fh.read())\n",
                        "                  ^^^^^^^^^\n",
                        "  File \"<frozen codecs>\", line 322, in decode\n",
                        "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 4: invalid start byte\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.03%)\n",
                        "\t-0.2004\t = Validation score   (-mean_squared_error)\n",
                        "\t2.14s\t = Training   runtime\n",
                        "\t0.25s\t = Validation runtime\n",
                        "Fitting model: RandomForest_BAG_L1 ... Training model for up to 310.89s of the 469.92s of remaining time.\n",
                        "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=32, gpus=0, mem=0.1/71.7 GB\n",
                        "\t-0.2513\t = Validation score   (-mean_squared_error)\n",
                        "\t1.79s\t = Training   runtime\n",
                        "\t0.47s\t = Validation runtime\n",
                        "Fitting model: CatBoost_BAG_L1 ... Training model for up to 308.34s of the 467.37s of remaining time.\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.53%)\n",
                        "\t-0.1896\t = Validation score   (-mean_squared_error)\n",
                        "\t69.95s\t = Training   runtime\n",
                        "\t0.04s\t = Validation runtime\n",
                        "Fitting model: XGBoost_BAG_L1 ... Training model for up to 235.10s of the 394.13s of remaining time.\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
                        "\t-0.2043\t = Validation score   (-mean_squared_error)\n",
                        "\t2.24s\t = Training   runtime\n",
                        "\t0.08s\t = Validation runtime\n",
                        "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 388.11s of remaining time.\n",
                        "\tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.0/71.2 GB\n",
                        "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.667, 'LightGBM_BAG_L1': 0.167, 'XGBoost_BAG_L1': 0.125, 'RandomForest_BAG_L1': 0.042}\n",
                        "\t-0.1874\t = Validation score   (-mean_squared_error)\n",
                        "\t0.04s\t = Training   runtime\n",
                        "\t0.0s\t = Validation runtime\n",
                        "Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
                        "Fitting model: LightGBM_BAG_L2 ... Training model for up to 388.03s of the 388.03s of remaining time.\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
                        "\t-0.1897\t = Validation score   (-mean_squared_error)\n",
                        "\t0.75s\t = Training   runtime\n",
                        "\t0.02s\t = Validation runtime\n",
                        "Fitting model: RandomForest_BAG_L2 ... Training model for up to 383.43s of the 383.42s of remaining time.\n",
                        "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=32, gpus=0, mem=0.1/71.6 GB\n",
                        "\t-0.1976\t = Validation score   (-mean_squared_error)\n",
                        "\t3.68s\t = Training   runtime\n",
                        "\t0.52s\t = Validation runtime\n",
                        "Fitting model: CatBoost_BAG_L2 ... Training model for up to 378.91s of the 378.91s of remaining time.\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.54%)\n",
                        "\t-0.1876\t = Validation score   (-mean_squared_error)\n",
                        "\t1.8s\t = Training   runtime\n",
                        "\t0.01s\t = Validation runtime\n",
                        "Fitting model: XGBoost_BAG_L2 ... Training model for up to 373.81s of the 373.80s of remaining time.\n",
                        "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.03%)\n",
                        "\t-0.1922\t = Validation score   (-mean_squared_error)\n",
                        "\t0.64s\t = Training   runtime\n",
                        "\t0.03s\t = Validation runtime\n",
                        "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 369.72s of remaining time.\n",
                        "\tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.0/70.7 GB\n",
                        "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.4, 'CatBoost_BAG_L1': 0.36, 'LightGBM_BAG_L2': 0.12, 'RandomForest_BAG_L2': 0.08, 'LightGBM_BAG_L1': 0.04}\n",
                        "\t-0.1857\t = Validation score   (-mean_squared_error)\n",
                        "\t0.05s\t = Training   runtime\n",
                        "\t0.0s\t = Validation runtime\n",
                        "AutoGluon training complete, total runtime = 107.24s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3941.9 rows/s (2064 batch size)\n",
                        "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\github\\DataScience\\scikit-learn\\scikit-learn\\ag_models_california_housing\")\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2592bc32850>"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Define the label column\n",
                "label = 'MedHouseVal'\n",
                "\n",
                "# Specify hyperparameters to ensure we get the desired base models\n",
                "# 'GBM' is LightGBM, 'CAT' is CatBoost, 'XGB' is XGBoost, 'RF' is Random Forest\n",
                "hyperparameters = {\n",
                "    'GBM': {},\n",
                "    'CAT': {},\n",
                "    'XGB': {},\n",
                "    'RF': {}\n",
                "}\n",
                "\n",
                "# Define the predictor path\n",
                "save_path = 'ag_models_california_housing'\n",
                "\n",
                "# Initialize Predictor\n",
                "predictor = TabularPredictor(label=label, path=save_path, problem_type='regression', eval_metric='mean_squared_error')\n",
                "\n",
                "# Fit models\n",
                "# presets='best_quality' enables Bagging and Stacking (L2/L3 ensembles)\n",
                "# time_limit can be adjusted as needed (e.g., 600 seconds for a quick run, or remove for full training)\n",
                "predictor.fit(\n",
                "    train_data=TabularDataset(train_df),\n",
                "    presets='best_quality',\n",
                "    hyperparameters=hyperparameters,\n",
                "    time_limit=600  # Set a time limit for demonstration (10 minutes)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation and Leaderboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                 model  score_test  score_val         eval_metric  \\\n",
                        "0  WeightedEnsemble_L3   -0.176687  -0.185744  mean_squared_error   \n",
                        "1      CatBoost_BAG_L1   -0.178364  -0.189618  mean_squared_error   \n",
                        "2  WeightedEnsemble_L2   -0.178425  -0.187358  mean_squared_error   \n",
                        "3      LightGBM_BAG_L2   -0.178680  -0.189710  mean_squared_error   \n",
                        "4      CatBoost_BAG_L2   -0.178698  -0.187564  mean_squared_error   \n",
                        "5       XGBoost_BAG_L2   -0.180692  -0.192206  mean_squared_error   \n",
                        "6      LightGBM_BAG_L1   -0.183305  -0.200409  mean_squared_error   \n",
                        "7  RandomForest_BAG_L2   -0.186302  -0.197647  mean_squared_error   \n",
                        "8       XGBoost_BAG_L1   -0.188759  -0.204334  mean_squared_error   \n",
                        "9  RandomForest_BAG_L1   -0.251320  -0.251288  mean_squared_error   \n",
                        "\n",
                        "   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n",
                        "0        1.694330       1.387620  82.391284                 0.003068   \n",
                        "1        0.161831       0.039309  69.945071                 0.161831   \n",
                        "2        1.308043       0.834951  76.161161                 0.002556   \n",
                        "3        1.328049       0.852897  76.869534                 0.022562   \n",
                        "4        1.323399       0.848780  77.914450                 0.017912   \n",
                        "5        1.381676       0.859273  76.759084                 0.076189   \n",
                        "6        0.437856       0.249700   2.143129                 0.437856   \n",
                        "7        1.650788       1.353284  79.794803                 0.345301   \n",
                        "8        0.360331       0.077340   2.240077                 0.360331   \n",
                        "9        0.345469       0.467580   1.790933                 0.345469   \n",
                        "\n",
                        "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
                        "0                0.000517           0.050916            3       True   \n",
                        "1                0.039309          69.945071            1       True   \n",
                        "2                0.001022           0.041951            2       True   \n",
                        "3                0.018968           0.750324            2       True   \n",
                        "4                0.014851           1.795240            2       True   \n",
                        "5                0.025344           0.639874            2       True   \n",
                        "6                0.249700           2.143129            1       True   \n",
                        "7                0.519356           3.675593            2       True   \n",
                        "8                0.077340           2.240077            1       True   \n",
                        "9                0.467580           1.790933            1       True   \n",
                        "\n",
                        "   fit_order  \n",
                        "0         10  \n",
                        "1          3  \n",
                        "2          5  \n",
                        "3          6  \n",
                        "4          8  \n",
                        "5          9  \n",
                        "6          1  \n",
                        "7          7  \n",
                        "8          4  \n",
                        "9          2  \n"
                    ]
                }
            ],
            "source": [
                "# Display the Leaderboard\n",
                "# This shows individual models (GBM, CAT, etc.) and Ensembles (WeightedEnsemble, Stacking)\n",
                "leaderboard = predictor.leaderboard(test_df)\n",
                "print(leaderboard)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Metrics: {'mean_squared_error': -0.17668715217642764, 'root_mean_squared_error': np.float64(-0.4203417088232235), 'mean_absolute_error': -0.2690656527236728, 'r2': 0.8651664059768475, 'pearsonr': 0.9302012429813988, 'median_absolute_error': -0.1696379127502441}\n"
                    ]
                }
            ],
            "source": [
                "# Evaluate on Test Data\n",
                "performance = predictor.evaluate(test_df)\n",
                "print(\"Test Metrics:\", performance)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Predictions and Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Computing feature importance via permutation shuffling for 8 features using 4128 rows with 5 shuffle sets...\n",
                        "\t65.01s\t= Expected runtime (13.0s per shuffle set)\n",
                        "\t13.29s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "            importance    stddev       p_value  n  p99_high   p99_low\n",
                        "Latitude      2.120570  0.020930  1.138578e-09  5  2.163664  2.077476\n",
                        "Longitude     1.946148  0.026099  3.880311e-09  5  1.999886  1.892410\n",
                        "MedInc        0.333517  0.007865  3.709104e-08  5  0.349712  0.317322\n",
                        "AveOccup      0.169404  0.003114  1.369990e-08  5  0.175817  0.162992\n",
                        "AveRooms      0.146086  0.006502  4.696100e-07  5  0.159473  0.132699\n",
                        "HouseAge      0.049270  0.003330  2.487598e-06  5  0.056126  0.042414\n",
                        "AveBedrms     0.012792  0.001733  3.946142e-05  5  0.016360  0.009223\n",
                        "Population    0.009867  0.000852  6.609502e-06  5  0.011622  0.008113\n"
                    ]
                }
            ],
            "source": [
                "# Feature Importance\n",
                "feature_importance = predictor.feature_importance(test_df)\n",
                "print(feature_importance)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MSE: 0.1767\n",
                        "MAE: 0.2691\n",
                        "R2: 0.8652\n"
                    ]
                }
            ],
            "source": [
                "# Make predictions\n",
                "y_pred = predictor.predict(test_df)\n",
                "y_test = test_df[label]\n",
                "\n",
                "# Calculate standard metrics manually if needed\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(f\"MSE: {mse:.4f}\")\n",
                "print(f\"MAE: {mae:.4f}\")\n",
                "print(f\"R2: {r2:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DS",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
