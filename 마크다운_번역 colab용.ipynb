{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gb3uQMSyXAZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator nbformat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPpSylzcXANF",
        "outputId": "e37c2948-87ea-40db-b696-8eb066d94a5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.26.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.9.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.30.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2026.1.4)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import nbformat\n",
        "import os\n",
        "from deep_translator import GoogleTranslator\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahG35SR6X4Pe",
        "outputId": "f11583f9-c24c-471b-a463-880529203274"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMriRIZ6W1S3",
        "outputId": "ba167034-2694-445b-97ec-54284959dac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ íŒŒì¼ í™•ì¸ ì™„ë£Œ: /content/drive/MyDrive/DataScience/a-data-science-framework-to-achieve-99-accuracy.ipynb\n",
            "ğŸ”„ ë²ˆì—­ ì‹œì‘... (DataScience í´ë” ë‚´ì—ì„œ ì‘ì—… ì¤‘)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Cells:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 22/58 [00:17<00:23,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš ï¸ ì…€ ë²ˆì—­ ê±´ë„ˆëœ€: <a id=\"ch7\"></a>\n",
            "# Step 5: Model Data\n",
            "Data Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But donâ€™t worry, we only need a high-level overview, which weâ€™ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, weâ€™ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, itâ€™s learning from us and not the other way around.\n",
            "\n",
            "Machine Learning (ML), as the name suggest, is teaching the machine how-to think and not what to think. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower, for businesses and professionals alike. This is both good and bad. Itâ€™s good because these algorithms are now accessible to more people that can solve more problems in the real-world. Itâ€™s bad because a lower barrier to entry means, more people will not know the tools they are using and can come to incorrect conclusions. Thatâ€™s why I focus on teaching you, not just what to do, but why youâ€™re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that Iâ€™ve hammered (no pun intended) my point, Iâ€™ll show you what to do and most importantly, WHY you do it.\n",
            "\n",
            "First, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.\n",
            "\n",
            "There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We'll save clustering and dimension reduction for another day, and focus on classification and regression. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the *sklearn* library to begin our analysis. We will use cross validation and scoring metrics, discussed in later sections, to rank and compare our algorithmsâ€™ performance.\n",
            "\n",
            "**Machine Learning Selection:**\n",
            "* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)\n",
            "* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)\n",
            "* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
            "* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
            "\n",
            "\n",
            "Now that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.\n",
            "\n",
            "**Machine Learning Classification Algorithms:**\n",
            "* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
            "* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
            "* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n",
            "* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n",
            "* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
            "* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n",
            "* [Discriminant Analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)\n",
            "\n",
            "\n",
            "### Data Science 101: How to Choose a Machine Learning Algorithm (MLA)\n",
            "**IMPORTANT:** When it comes to data modeling, the beginnerâ€™s question is always, \"what is the best machine learning algorithm?\" To this the beginner must learn, the [No Free Lunch Theorem (NFLT)](http://robertmarks.org/Classes/ENGR5358/Papers/NFL_4_Dummies.pdf) of Machine Learning. In short, NFLT states, there is no super algorithm, that works best in all situations, for all datasets. So the best approach is to try multiple MLAs, tune them, and compare them for your specific scenario. With that being said, some good research has been done to compare algorithms, such as [Caruana & Niculescu-Mizil 2006](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) watch [video lecture here](http://videolectures.net/solomon_caruana_wslmw/) of MLA comparisons, [Ogutu et al. 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3103196/) done by the NIH for genomic selection, [Fernandez-Delgado et al. 2014](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) comparing 179 classifiers from 17 families, [Thoma 2016 sklearn comparison](https://martin-thoma.com/comparing-classifiers/), and there is also a school of thought that says, [more data beats a better algorithm](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html). \n",
            "\n",
            "So with all this information, where is a beginner to start? I recommend starting with [Trees, Bagging, Random Forests, and Boosting](http://jessica2.msri.org/attachments/10778/10778-boost.pdf). They are basically different implementations of a decision tree, which is the easiest concept to learn and understand. They are also easier to tune, discussed in the next section, than something like SVC. Below, I'll give an overview of how-to run and compare several MLAs, but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives. --> Text length need to be between 0 and 5000 characters\n",
            "\n",
            "âš ï¸ ì…€ ë²ˆì—­ ê±´ë„ˆëœ€: <a id=\"ch8\"></a>\n",
            "## 5.1 Evaluate Model Performance\n",
            "Let's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind when improving your model.\n",
            "\n",
            "### Data Science 101: Determine a Baseline Accuracy ###\n",
            "Before we decide how-to make our model better, let's determine if our model is even worth keeping. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip problem. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing correct. So, let's set 50% as the worst model performance; because anything lower than that, then why do I need you when I can just flip a coin?\n",
            "\n",
            "Okay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, if we just predict the most frequent occurrence, that 100% of people died, then we would be right 67.5% of the time. So, let's set 68% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.\n",
            "\n",
            "### Data Science 101: How-to Create Your Own Model ###\n",
            "Our accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that segment your target response, placing the survived/1 and dead/0 into homogeneous subgroups. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n",
            "\n",
            "Remember, the name of the game is to create subgroups using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is less than 10 and/or our model accuracy plateaus or decreases. Got it? Let's go!\n",
            "\n",
            "***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n",
            "\n",
            "***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n",
            "\n",
            "***Question 3A (going down the female branch with count = 314): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n",
            "\n",
            "***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. \n",
            "\n",
            "***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n",
            "\n",
            "***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n",
            "\n",
            "You did it, with very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model? \n",
            "\n",
            "Before we do, let's code what we just wrote above. Please note, this is a manual process created by \"hand.\" You won't have to do this, but it's important to understand it before you start working with MLA. Think of MLA like a TI-89 calculator on a Calculus Exam. It's very powerful and helps you with a lot of the grunt work. But if you don't know what you're doing on the exam, a calculator, even a TI-89, is not going to help you pass. So, study the next section wisely.\n",
            "\n",
            "Reference: [Cross-Validation and Decision Tree Tutorial](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf) --> Text length need to be between 0 and 5000 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Cells: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:25<00:00,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… ì‘ì—… ì™„ë£Œ! ì´ 20/23ê°œì˜ ë§ˆí¬ë‹¤ìš´ ì…€ì´ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "ğŸ’¾ ì €ì¥ ìœ„ì¹˜: /content/drive/MyDrive/DataScience/a-data-science-framework-to-achieve-99-accuracy_translated.ipynb\n",
            "ğŸ‘‰ êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ 'DataScience' í´ë”ì—ì„œ '_translated' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "folder_name = 'DataScience'  # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ í´ë” ì´ë¦„\n",
        "filename = \"a-data-science-framework-to-achieve-99-accuracy.ipynb\" # íŒŒì¼ ì´ë¦„\n",
        "\n",
        "# ì „ì²´ ê²½ë¡œ ìƒì„± (/content/drive/MyDrive/í´ë”ëª…/íŒŒì¼ëª…)\n",
        "input_path = os.path.join('/content/drive/MyDrive', folder_name, filename)\n",
        "output_filename = filename.replace(\".ipynb\", \"_translated.ipynb\")\n",
        "output_path = os.path.join('/content/drive/MyDrive', folder_name, output_filename)\n",
        "\n",
        "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
        "if not os.path.exists(input_path):\n",
        "    print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ğŸ‘‰ í™•ì¸ëœ ê²½ë¡œ: {input_path}\")\n",
        "    print(f\"ğŸ’¡ íŒ: êµ¬ê¸€ ë“œë¼ì´ë¸Œì— '{folder_name}' í´ë”ê°€ ìˆëŠ”ì§€, íŒŒì¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "else:\n",
        "    print(f\"ğŸ“‚ íŒŒì¼ í™•ì¸ ì™„ë£Œ: {input_path}\")\n",
        "\n",
        "    # 4ë‹¨ê³„: ë…¸íŠ¸ë¶ íŒŒì¼ ì½ê¸°\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "    # 5ë‹¨ê³„: ë²ˆì—­ê¸° ì„¤ì • ë° ì‹¤í–‰\n",
        "    translator = GoogleTranslator(source='auto', target='ko')\n",
        "    print(\"ğŸ”„ ë²ˆì—­ ì‹œì‘... (DataScience í´ë” ë‚´ì—ì„œ ì‘ì—… ì¤‘)\")\n",
        "\n",
        "    count = 0\n",
        "    total_markdown = sum(1 for cell in nb.cells if cell.cell_type == 'markdown')\n",
        "\n",
        "    # ì§„í–‰ë¥  í‘œì‹œì¤„(tqdm)ê³¼ í•¨ê»˜ ë²ˆì—­ ìˆ˜í–‰\n",
        "    for cell in tqdm(nb.cells, desc=\"Processing Cells\"):\n",
        "        if cell.cell_type == 'markdown':\n",
        "            original_text = cell.source\n",
        "            if original_text.strip():\n",
        "                try:\n",
        "                    # ë²ˆì—­ ìˆ˜í–‰\n",
        "                    translated_text = translator.translate(original_text)\n",
        "                    cell.source = translated_text\n",
        "                    count += 1\n",
        "                except Exception as e:\n",
        "                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€í•˜ê³  ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥\n",
        "                    print(f\"\\nâš ï¸ ì…€ ë²ˆì—­ ê±´ë„ˆëœ€: {e}\")\n",
        "                    continue\n",
        "\n",
        "    # 6ë‹¨ê³„: ë²ˆì—­ëœ íŒŒì¼ ì €ì¥ (ê°™ì€ í´ë”ì— ì €ì¥)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        nbformat.write(nb, f)\n",
        "\n",
        "    print(f\"\\nâœ… ì‘ì—… ì™„ë£Œ! ì´ {count}/{total_markdown}ê°œì˜ ë§ˆí¬ë‹¤ìš´ ì…€ì´ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n",
        "    print(f\"ğŸ‘‰ êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ '{folder_name}' í´ë”ì—ì„œ '_translated' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
      ]
    }
  ]
}