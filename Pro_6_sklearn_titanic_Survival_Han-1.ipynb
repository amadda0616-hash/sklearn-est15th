{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pro_6 FIXED - ÌÉÄÏù¥ÌÉÄÎãâ ÏÉùÏ°¥Ïûê ÏòàÏ∏°\n",
                "\n",
                "## üéØ Major Updates\n",
                "1. **Sex Feature Restored**: Kept `Sex` (only removed `Sex_Pclass`)\n",
                "2. **Expanded Ensemble**: 4 models (XGBoost, RF, SVC, GB)\n",
                "3. **Improved Validation**: RepeatedStratifiedKFold CV\n",
                "4. **Threshold Optimization**: Finding optimal probability threshold\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ Î∞è ÏÑ§Ï†ï\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import platform\n",
                "import os\n",
                "\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, cross_val_predict\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.impute import KNNImputer\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.feature_selection import RFECV\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
                "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "\n",
                "import optuna\n",
                "from optuna.samplers import TPESampler\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "\n",
                "# Visualization settings\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "plt.rcParams['axes.labelsize'] = 12\n",
                "plt.rcParams['axes.titlesize'] = 14\n",
                "\n",
                "# Korean Font Settings\n",
                "system_name = platform.system()\n",
                "if system_name == 'Windows':\n",
                "    print('ü™ü Windows: Malgun Gothic ÏÑ§Ï†ï')\n",
                "    plt.rc('font', family='Malgun Gothic')\n",
                "elif system_name == 'Darwin': \n",
                "    print('üçé Mac: AppleGothic ÏÑ§Ï†ï')\n",
                "    plt.rc('font', family='AppleGothic')\n",
                "else:\n",
                "    print('üêß Linux/Other: NanumGothic ÏÑ§Ï†ï')\n",
                "    plt.rc('font', family='NanumGothic')\n",
                "\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "print('‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ Î∞è ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ÏôÑÎ£å')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
                "# ============================================================\n",
                "base_path = r'C:/Users/user/github/DataScience/scikit-learn/scikit-learn/data/titanic'\n",
                "train_df = pd.read_csv(f'{base_path}/train.csv')\n",
                "test_df = pd.read_csv(f'{base_path}/test.csv')\n",
                "\n",
                "test_passenger_ids = test_df['PassengerId'].copy()\n",
                "train_len = len(train_df)\n",
                "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
                "\n",
                "print(f'Train: {train_df.shape}, Test: {test_df.shape}, All: {all_data.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 3. KNN Imputer for Age, Fare, Embarked\n",
                "# ============================================================\n",
                "def find_best_k_neighbors(train_df):\n",
                "    print('üîç KNN Imputer n_neighbors ÏµúÏ†ÅÌôî Ï§ë...')\n",
                "    results = []\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
                "    \n",
                "    for k in range(3, 12, 2):\n",
                "        df = train_df.copy()\n",
                "        df['Sex_num'] = (df['Sex'] == 'male').astype(int)\n",
                "        df['Embarked_num'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).fillna(-1)\n",
                "        \n",
                "        imputer_cols = ['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_num']\n",
                "        imputer = KNNImputer(n_neighbors=k)\n",
                "        df_imputed = pd.DataFrame(imputer.fit_transform(df[imputer_cols]), columns=imputer_cols)\n",
                "        df['Age'] = df_imputed['Age']\n",
                "        df['Fare'] = df_imputed['Fare']\n",
                "        df['Embarked_num'] = df_imputed['Embarked_num'].round().astype(int)\n",
                "        \n",
                "        X = df[['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_num']]\n",
                "        y = df['Survived'].astype(int)\n",
                "        model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1)\n",
                "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
                "        results.append({'k': k, 'cv_accuracy': scores.mean()})\n",
                "        print(f'  k={k}: CV Accuracy = {scores.mean():.4f}')\n",
                "    \n",
                "    results_df = pd.DataFrame(results)\n",
                "    best_k = results_df.loc[results_df['cv_accuracy'].idxmax(), 'k']\n",
                "    print(f'\\nüèÜ ÏµúÏ†Å n_neighbors = {best_k}')\n",
                "    return int(best_k)\n",
                "\n",
                "best_k = find_best_k_neighbors(train_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 4. WCG (Women, Children, Group) Ï†ÑÎûµ + KNN Imputer Ï†ÅÏö©\n",
                "# ============================================================\n",
                "def add_wcg_family_survival(all_data, train_len, best_k):\n",
                "    all_data = all_data.copy()\n",
                "    \n",
                "    all_data['Last_Name'] = all_data['Name'].apply(lambda x: x.split(',')[0])\n",
                "    all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
                "    all_data['TicketFrequency'] = all_data['Ticket'].map(all_data['Ticket'].value_counts())\n",
                "    all_data['Sex_num'] = (all_data['Sex'] == 'male').astype(int)\n",
                "    all_data['Embarked_num'] = all_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
                "    \n",
                "    print(f'üîß KNN Imputer Ï†ÅÏö© (n_neighbors={best_k})...')\n",
                "    imputer_cols = ['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_num']\n",
                "    imputer = KNNImputer(n_neighbors=best_k)\n",
                "    df_imputed = pd.DataFrame(imputer.fit_transform(all_data[imputer_cols]), columns=imputer_cols, index=all_data.index)\n",
                "    all_data['Age'] = df_imputed['Age']\n",
                "    all_data['Fare'] = df_imputed['Fare']\n",
                "    all_data['Embarked_num'] = df_imputed['Embarked_num'].round().astype(int)\n",
                "    \n",
                "    all_data['Fare_Per_Person'] = all_data['Fare'] / all_data['TicketFrequency']\n",
                "    all_data['Fare_Per_Person_Round'] = all_data['Fare_Per_Person'].round(2)\n",
                "    \n",
                "    all_data['IsChild'] = (all_data['Age'] < 10).astype(int)\n",
                "    all_data['IsFemale'] = (all_data['Sex'] == 'female').astype(int)\n",
                "    all_data['IsMaster'] = (all_data['Title'] == 'Master').astype(int)\n",
                "    all_data['IsMaleChild'] = ((all_data['Sex'] == 'male') & (all_data['IsChild'] == 1)).astype(int)\n",
                "    \n",
                "    all_data['Family_Survival'] = 0.5\n",
                "    all_data['WCG_Survival'] = 0\n",
                "    \n",
                "    print('üìä Family_Survival Í≥ÑÏÇ∞ Ï§ë...')\n",
                "    for ticket, grp_df in all_data.groupby('Ticket'):\n",
                "        if len(grp_df) > 1:\n",
                "            for idx in grp_df.index:\n",
                "                others = grp_df.drop(idx)\n",
                "                others_train = others[others.index < train_len]\n",
                "                if len(others_train) > 0:\n",
                "                    if others_train['Survived'].max() == 1.0:\n",
                "                        all_data.loc[idx, 'Family_Survival'] = 1\n",
                "                    elif others_train['Survived'].min() == 0.0:\n",
                "                        all_data.loc[idx, 'Family_Survival'] = 0\n",
                "    \n",
                "    for (last_name, fare_pp, embarked), grp_df in all_data.groupby(['Last_Name', 'Fare_Per_Person_Round', 'Embarked_num']):\n",
                "        if len(grp_df) > 1:\n",
                "            for idx in grp_df.index:\n",
                "                if all_data.loc[idx, 'Family_Survival'] == 0.5:\n",
                "                    others = grp_df.drop(idx)\n",
                "                    others_train = others[others.index < train_len]\n",
                "                    if len(others_train) > 0:\n",
                "                        if others_train['Survived'].max() == 1.0:\n",
                "                            all_data.loc[idx, 'Family_Survival'] = 1\n",
                "                        elif others_train['Survived'].min() == 0.0:\n",
                "                            all_data.loc[idx, 'Family_Survival'] = 0\n",
                "    \n",
                "    print('üéØ WCG Ï†ÑÎûµ Ï†ÅÏö© Ï§ë...')\n",
                "    for ticket, grp_df in all_data.groupby('Ticket'):\n",
                "        if len(grp_df) > 1:\n",
                "            grp_train = grp_df[grp_df.index < train_len]\n",
                "            if len(grp_train) > 0:\n",
                "                women_children = grp_train[(grp_train['IsFemale'] == 1) | (grp_train['IsChild'] == 1)]\n",
                "                if len(women_children) > 0 and (women_children['Survived'] == 1).all():\n",
                "                    for idx in grp_df.index:\n",
                "                        if all_data.loc[idx, 'IsMaster'] == 1 or all_data.loc[idx, 'IsMaleChild'] == 1:\n",
                "                            all_data.loc[idx, 'WCG_Survival'] = 1\n",
                "                            all_data.loc[idx, 'Family_Survival'] = 1\n",
                "    \n",
                "    print(f'‚úÖ Family_Survival Î∂ÑÌè¨: {all_data[\"Family_Survival\"].value_counts().to_dict()}')\n",
                "    return all_data\n",
                "\n",
                "all_data = add_wcg_family_survival(all_data, train_len, best_k)\n",
                "print('\\n‚úÖ KNN Imputer + WCG ÌäπÏÑ± ÏÉùÏÑ± ÏôÑÎ£å')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5. Feature Engineering\n",
                "# ============================================================\n",
                "def preprocessing_pro5(all_data):\n",
                "    all_data = all_data.copy()\n",
                "    \n",
                "    title_mapping = {'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
                "        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Capt': 'Rare',\n",
                "        'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
                "        'Lady': 'Rare', 'Countess': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Jonkheer': 'Rare'}\n",
                "    all_data['Title'] = all_data['Title'].map(title_mapping).fillna('Rare')\n",
                "    all_data['Sex'] = (all_data['Sex'] == 'male').astype(int)\n",
                "    all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n",
                "    all_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)\n",
                "    \n",
                "    def age_to_bin(age):\n",
                "        if pd.isna(age): return 2\n",
                "        elif age < 10: return 0\n",
                "        elif age < 18: return 1\n",
                "        elif age < 35: return 2\n",
                "        elif age < 50: return 3\n",
                "        else: return 4\n",
                "    all_data['AgeBin'] = all_data['Age'].apply(age_to_bin).astype(int)\n",
                "    all_data['LogFare'] = np.log1p(all_data['Fare_Per_Person'])\n",
                "    all_data['Deck'] = all_data['Cabin'].str[0].fillna('U')\n",
                "    deck_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 0, 'U': 0}\n",
                "    all_data['Deck'] = all_data['Deck'].map(deck_map).fillna(0).astype(int)\n",
                "    title_enc = {'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4}\n",
                "    all_data['Title'] = all_data['Title'].map(title_enc).fillna(4).astype(int)\n",
                "    all_data['Sex_Pclass'] = all_data['Sex'] * all_data['Pclass']\n",
                "    \n",
                "    return all_data\n",
                "\n",
                "all_data = preprocessing_pro5(all_data)\n",
                "print('‚úÖ Pro5 Feature Engineering ÏôÑÎ£å')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.5 Pro_6 Additional Feature Engineering\n",
                "# ============================================================\n",
                "def add_pro6_features(all_data):\n",
                "    all_data = all_data.copy()\n",
                "    \n",
                "    all_data['FareBin'] = pd.qcut(\n",
                "        all_data['Fare_Per_Person'].fillna(all_data['Fare_Per_Person'].median()), \n",
                "        q=5, labels=[0,1,2,3,4], duplicates='drop'\n",
                "    ).astype(int)\n",
                "    \n",
                "    def familysize_to_bin(size):\n",
                "        if size == 1: return 0\n",
                "        elif size <= 3: return 1\n",
                "        else: return 2\n",
                "    all_data['FamilySizeBin'] = all_data['FamilySize'].apply(familysize_to_bin).astype(int)\n",
                "    \n",
                "    return all_data\n",
                "\n",
                "all_data = add_pro6_features(all_data)\n",
                "print('‚úÖ Pro6 Ï∂îÍ∞Ä ÌäπÏÑ± ÏÉùÏÑ± ÏôÑÎ£å')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 6. Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨ Î∞è ÌäπÏÑ± Ï†ïÏùò [FIXED: Sex Ïú†ÏßÄ]\n",
                "# ============================================================\n",
                "features = ['Pclass', 'Sex', 'FareBin', 'Embarked_num', 'Title', 'FamilySizeBin', \n",
                "            'Family_Survival', 'TicketFrequency', 'Deck', 'AgeBin', 'Sex_Pclass']\n",
                "\n",
                "print(f\"üìå Ï¥àÍ∏∞ ÌäπÏÑ± ({len(features)}Í∞ú): {features}\")\n",
                "\n",
                "train_processed = all_data.iloc[:train_len].copy()\n",
                "test_processed = all_data.iloc[train_len:].copy()\n",
                "\n",
                "X = train_processed[features]\n",
                "y = train_processed['Survived'].astype(int)\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
                "cv_optuna = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "print(f'Full Dataset: {X.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 6.5 VIF Analysis [FIXED: Sex_PclassÎßå Ï†úÍ±∞]\n",
                "# ============================================================\n",
                "def calculate_vif(df, features_list):\n",
                "    vif_data = pd.DataFrame()\n",
                "    vif_data[\"Feature\"] = features_list\n",
                "    vif_data[\"VIF\"] = [variance_inflation_factor(df[features_list].values, i) \n",
                "                       for i in range(len(features_list))]\n",
                "    return vif_data.sort_values('VIF', ascending=False)\n",
                "\n",
                "print(\"üîç VIF Analysis:\")\n",
                "vif_df = calculate_vif(X, features)\n",
                "print(vif_df)\n",
                "\n",
                "# FIXED: Only remove Sex_Pclass, keep Sex!\n",
                "high_vif_to_remove = ['Sex_Pclass']\n",
                "removed = [f for f in high_vif_to_remove if f in features]\n",
                "if removed:\n",
                "    print(f\"\\n‚ö†Ô∏è ÍµêÌò∏ÏûëÏö©Ìï≠Îßå Ï†úÍ±∞: {removed}\")\n",
                "    features = [f for f in features if f not in removed]\n",
                "    X = X[features]\n",
                "    print(\"‚úÖ VIF Ïû¨Í≤ÄÏ¶ù:\")\n",
                "    print(calculate_vif(X, features))\n",
                "\n",
                "print(f\"\\nüìå ÏµúÏ¢Ö ÌäπÏÑ± ({len(features)}Í∞ú): {features}\")\n",
                "print(f\"üî• Sex Ìè¨Ìï®: {'Sex' in features}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 7. Baseline Models\n",
                "# ============================================================\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "baseline_models = {\n",
                "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
                "    'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
                "    'SVC': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
                "    'KNeighbors': KNeighborsClassifier(),\n",
                "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
                "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=1.6)\n",
                "}\n",
                "\n",
                "print('\\nüöÄ Baseline Models CV:')\n",
                "for name, model in baseline_models.items():\n",
                "    X_use = X_scaled if name in ['SVC', 'KNeighbors', 'LogisticRegression'] else X\n",
                "    scores = cross_val_score(model, X_use, y, cv=cv, scoring='accuracy')\n",
                "    print(f'  {name}: {scores.mean():.4f} ¬± {scores.std():.4f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 8. Optuna Helper Class\n",
                "# ============================================================\n",
                "class AdaptiveOverfittingMonitor:\n",
                "    def __init__(self):\n",
                "        self.reset()\n",
                "\n",
                "    def adjust_for_overfitting(self):\n",
                "        print(\"‚ö†Ô∏è Í≥ºÏ†ÅÌï© Í∞êÏßÄ! Í∑úÏ†ú Í∞ïÌôî...\")\n",
                "        if self.max_depth_range[1] > 3:\n",
                "            self.max_depth_range[1] -= 1\n",
                "        self.subsample_range[0] = min(0.9, self.subsample_range[0] + 0.05)\n",
                "        self.reg_alpha_min *= 5\n",
                "        self.reg_lambda_min *= 5\n",
                "        return True\n",
                "    \n",
                "    def reset(self):\n",
                "        self.max_depth_range = [3, 10]\n",
                "        self.subsample_range = [0.6, 1.0]\n",
                "        self.reg_alpha_min = 1e-5\n",
                "        self.reg_lambda_min = 1e-5\n",
                "\n",
                "adaptive_params = AdaptiveOverfittingMonitor()\n",
                "\n",
                "def check_overfitting(model, X_train, y_train, name, cv, threshold=0.03):\n",
                "    train_score = accuracy_score(y_train, model.predict(X_train))\n",
                "    cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
                "    gap = train_score - cv_score\n",
                "    print(f\"‚úÖ {name}: Train={train_score:.4f}, CV={cv_score:.4f}, Gap={gap:.4f}\")\n",
                "    return gap > threshold, train_score, cv_score, gap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 9. Tune XGBoost\n",
                "# ============================================================\n",
                "def tune_xgb(X_train, y_train, cv, cv_optuna, adaptive_params):\n",
                "    adaptive_params.reset()\n",
                "    for iteration in range(2):\n",
                "        print(f'\\n{\"=\"*50}\\nüöÄ XGBoost ÌäúÎãù #{iteration + 1}\\n{\"=\"*50}')\n",
                "        def objective(trial):\n",
                "            params = {\n",
                "                'scale_pos_weight': 1.6,\n",
                "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "                'max_depth': trial.suggest_int('max_depth', adaptive_params.max_depth_range[0], adaptive_params.max_depth_range[1]),\n",
                "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
                "                'subsample': trial.suggest_float('subsample', adaptive_params.subsample_range[0], adaptive_params.subsample_range[1]),\n",
                "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
                "                'reg_alpha': trial.suggest_float('reg_alpha', adaptive_params.reg_alpha_min, 10.0, log=True),\n",
                "                'reg_lambda': trial.suggest_float('reg_lambda', adaptive_params.reg_lambda_min, 10.0, log=True),\n",
                "                'random_state': 42, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1\n",
                "            }\n",
                "            return cross_val_score(XGBClassifier(**params), X_train, y_train, cv=cv_optuna, scoring='accuracy').mean()\n",
                "        \n",
                "        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42+iteration))\n",
                "        study.optimize(objective, n_trials=30, timeout=120, show_progress_bar=True)\n",
                "        print(f'Best CV: {study.best_value:.4f}')\n",
                "        \n",
                "        best_model = XGBClassifier(**study.best_params, scale_pos_weight=1.6, random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
                "        best_model.fit(X_train, y_train)\n",
                "        \n",
                "        is_overfitting, _, _, _ = check_overfitting(best_model, X_train, y_train, 'XGBoost', cv)\n",
                "        if not is_overfitting: return study, best_model\n",
                "        adaptive_params.adjust_for_overfitting()\n",
                "    return study, best_model\n",
                "\n",
                "study_xgb, best_xgb = tune_xgb(X, y, cv, cv_optuna, adaptive_params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 10. Tune RandomForest\n",
                "# ============================================================\n",
                "def tune_rf(X_train, y_train, cv, cv_optuna, adaptive_params):\n",
                "    adaptive_params.reset()\n",
                "    for iteration in range(2):\n",
                "        print(f'\\n{\"=\"*50}\\nüöÄ RandomForest ÌäúÎãù #{iteration + 1}\\n{\"=\"*50}')\n",
                "        def objective(trial):\n",
                "            params = {\n",
                "                'class_weight': 'balanced',\n",
                "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "                'max_depth': trial.suggest_int('max_depth', adaptive_params.max_depth_range[0], adaptive_params.max_depth_range[1] + 2),\n",
                "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
                "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
                "                'random_state': 42, 'n_jobs': -1\n",
                "            }\n",
                "            return cross_val_score(RandomForestClassifier(**params), X_train, y_train, cv=cv_optuna, scoring='accuracy').mean()\n",
                "        \n",
                "        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42+iteration))\n",
                "        study.optimize(objective, n_trials=30, timeout=120, show_progress_bar=True)\n",
                "        print(f'Best CV: {study.best_value:.4f}')\n",
                "        \n",
                "        best_model = RandomForestClassifier(**study.best_params, class_weight='balanced', random_state=42, n_jobs=-1)\n",
                "        best_model.fit(X_train, y_train)\n",
                "        \n",
                "        is_overfitting, _, _, _ = check_overfitting(best_model, X_train, y_train, 'RandomForest', cv)\n",
                "        if not is_overfitting: return study, best_model\n",
                "        adaptive_params.adjust_for_overfitting()\n",
                "    return study, best_model\n",
                "\n",
                "study_rf, best_rf = tune_rf(X, y, cv, cv_optuna, adaptive_params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 11. Tune SVC [NEW]\n",
                "# ============================================================\n",
                "def tune_svc(X_train, y_train, cv, cv_optuna):\n",
                "    print(f'\\n{\"=\"*50}\\nüöÄ SVC ÌäúÎãù\\n{\"=\"*50}')\n",
                "    def objective(trial):\n",
                "        params = {\n",
                "            'C': trial.suggest_float('C', 0.1, 10.0, log=True),\n",
                "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
                "            'kernel': trial.suggest_categorical('kernel', ['rbf', 'poly']),\n",
                "            'class_weight': 'balanced',\n",
                "            'probability': True,\n",
                "            'random_state': 42\n",
                "        }\n",
                "        return cross_val_score(SVC(**params), X_train, y_train, cv=cv_optuna, scoring='accuracy').mean()\n",
                "    \n",
                "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
                "    study.optimize(objective, n_trials=20, timeout=60, show_progress_bar=True)\n",
                "    print(f'Best CV: {study.best_value:.4f}')\n",
                "    \n",
                "    best_model = SVC(**study.best_params, class_weight='balanced', probability=True, random_state=42)\n",
                "    best_model.fit(X_train, y_train)\n",
                "    check_overfitting(best_model, X_train, y_train, 'SVC', cv)\n",
                "    return study, best_model\n",
                "\n",
                "study_svc, best_svc = tune_svc(X_scaled, y, cv, cv_optuna)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 12. Tune GradientBoosting [NEW]\n",
                "# ============================================================\n",
                "def tune_gb(X_train, y_train, cv, cv_optuna):\n",
                "    print(f'\\n{\"=\"*50}\\nüöÄ GradientBoosting ÌäúÎãù\\n{\"=\"*50}')\n",
                "    def objective(trial):\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
                "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
                "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
                "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
                "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
                "            'random_state': 42\n",
                "        }\n",
                "        return cross_val_score(GradientBoostingClassifier(**params), X_train, y_train, cv=cv_optuna, scoring='accuracy').mean()\n",
                "    \n",
                "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
                "    study.optimize(objective, n_trials=30, timeout=120, show_progress_bar=True)\n",
                "    print(f'Best CV: {study.best_value:.4f}')\n",
                "    \n",
                "    best_model = GradientBoostingClassifier(**study.best_params, random_state=42)\n",
                "    best_model.fit(X_train, y_train)\n",
                "    check_overfitting(best_model, X_train, y_train, 'GradientBoosting', cv)\n",
                "    return study, best_model\n",
                "\n",
                "study_gb, best_gb = tune_gb(X, y, cv, cv_optuna)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 13. Threshold Optimization [FIXED: StratifiedKFold ÏÇ¨Ïö©]\n",
                "# ============================================================\n",
                "print(f'\\n{\"=\"*50}\\nüéØ Threshold ÏµúÏ†ÅÌôî\\n{\"=\"*50}')\n",
                "\n",
                "# FIXED: Use StratifiedKFold (not Repeated) for cross_val_predict\n",
                "cv_predict = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "probs = cross_val_predict(best_xgb, X, y, cv=cv_predict, method='predict_proba')[:, 1]\n",
                "\n",
                "best_threshold = 0.5\n",
                "best_acc = 0\n",
                "for t in np.arange(0.35, 0.65, 0.05):\n",
                "    acc = accuracy_score(y, (probs >= t).astype(int))\n",
                "    print(f\"  Threshold {t:.2f}: Accuracy = {acc:.4f}\")\n",
                "    if acc > best_acc:\n",
                "        best_acc = acc\n",
                "        best_threshold = t\n",
                "\n",
                "print(f\"\\nüèÜ ÏµúÏ†Å Threshold: {best_threshold:.2f} (Accuracy: {best_acc:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 14. Ensemble (4 Models)\n",
                "# ============================================================\n",
                "print(f'\\n{\"=\"*50}\\nüî• 4-Model Ensemble\\n{\"=\"*50}')\n",
                "\n",
                "class ScaledSVC(BaseEstimator):\n",
                "    def __init__(self, svc_model, scaler):\n",
                "        self.svc_model = svc_model\n",
                "        self.scaler = scaler\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        X_scaled = self.scaler.fit_transform(X)\n",
                "        self.svc_model.fit(X_scaled, y)\n",
                "        return self\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return self.svc_model.predict(self.scaler.transform(X))\n",
                "    \n",
                "    def predict_proba(self, X):\n",
                "        return self.svc_model.predict_proba(self.scaler.transform(X))\n",
                "\n",
                "scaled_svc = ScaledSVC(best_svc, StandardScaler())\n",
                "scaled_svc.fit(X, y)\n",
                "\n",
                "tuned_models = {\n",
                "    'XGBoost': best_xgb,\n",
                "    'RandomForest': best_rf,\n",
                "    'SVC': scaled_svc,\n",
                "    'GradientBoosting': best_gb\n",
                "}\n",
                "\n",
                "print('\\nüìä Tuned Models CV:')\n",
                "for name, model in tuned_models.items():\n",
                "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
                "    print(f'  {name}: {scores.mean():.4f} ¬± {scores.std():.4f}')\n",
                "\n",
                "estimators = [(name, model) for name, model in tuned_models.items()]\n",
                "\n",
                "# Soft Voting\n",
                "voting_soft = VotingClassifier(estimators=estimators, voting='soft')\n",
                "voting_soft.fit(X, y)\n",
                "soft_cv = cross_val_score(voting_soft, X, y, cv=cv, scoring='accuracy')\n",
                "print(f'\\nVoting (Soft) CV: {soft_cv.mean():.4f} ¬± {soft_cv.std():.4f}')\n",
                "\n",
                "# Hard Voting\n",
                "voting_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
                "voting_hard.fit(X, y)\n",
                "hard_cv = cross_val_score(voting_hard, X, y, cv=cv, scoring='accuracy')\n",
                "print(f'Voting (Hard) CV: {hard_cv.mean():.4f} ¬± {hard_cv.std():.4f}')\n",
                "\n",
                "# Stacking\n",
                "stacking = StackingClassifier(\n",
                "    estimators=estimators, \n",
                "    final_estimator=LogisticRegression(C=0.1, max_iter=1000, class_weight='balanced'), \n",
                "    cv=5, n_jobs=-1\n",
                ")\n",
                "stacking.fit(X, y)\n",
                "stack_cv = cross_val_score(stacking, X, y, cv=cv, scoring='accuracy')\n",
                "print(f'Stacking CV: {stack_cv.mean():.4f} ¬± {stack_cv.std():.4f}')\n",
                "\n",
                "ensemble_results = {'Voting_Soft': soft_cv.mean(), 'Voting_Hard': hard_cv.mean(), 'Stacking': stack_cv.mean()}\n",
                "best_ensemble_name = max(ensemble_results, key=ensemble_results.get)\n",
                "print(f'\\nüèÜ Best Ensemble: {best_ensemble_name} (CV: {ensemble_results[best_ensemble_name]:.4f})')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 15. ÏµúÏ¢Ö Ï†úÏ∂ú\n",
                "# ============================================================\n",
                "print(f'\\n{\"=\"*50}\\nüìù ÏµúÏ¢Ö Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ±\\n{\"=\"*50}')\n",
                "\n",
                "X_test_final = test_processed[features]\n",
                "output_path = r'C:/Users/user/github/DataScience/scikit-learn/scikit-learn/Submission'\n",
                "os.makedirs(output_path, exist_ok=True)\n",
                "\n",
                "best_model_map = {'Voting_Soft': voting_soft, 'Voting_Hard': voting_hard, 'Stacking': stacking}\n",
                "best_model = best_model_map[best_ensemble_name]\n",
                "\n",
                "for name, model, use_threshold in [\n",
                "    ('Pro6_Fixed_Voting_Soft', voting_soft, True),\n",
                "    ('Pro6_Fixed_Voting_Hard', voting_hard, False),\n",
                "    ('Pro6_Fixed_Stacking', stacking, True),\n",
                "    (f'Pro6_Fixed_Best_{best_ensemble_name}', best_model, best_ensemble_name != 'Voting_Hard')\n",
                "]:\n",
                "    save_path = f'{output_path}/submission_{name}.csv'\n",
                "    \n",
                "    if use_threshold and hasattr(model, 'predict_proba'):\n",
                "        probs = model.predict_proba(X_test_final)[:, 1]\n",
                "        pred = (probs >= best_threshold).astype(int)\n",
                "        print(f'‚ÑπÔ∏è Threshold {best_threshold:.2f} for {name}')\n",
                "    else:\n",
                "        pred = model.predict(X_test_final).astype(int)\n",
                "        \n",
                "    pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': pred}).to_csv(save_path, index=False)\n",
                "    print(f'‚úÖ {save_path} (Survived: {pred.sum()}/{len(pred)})')\n",
                "\n",
                "print('\\nüéâ Pro_6 FIXED ÏôÑÎ£å!')\n",
                "print(f'  1. ‚úÖ Sex ÌäπÏÑ± Î≥µÏõê')\n",
                "print(f'  2. ‚úÖ 4Í∞ú Î™®Îç∏ ÏïôÏÉÅÎ∏î')\n",
                "print(f'  3. ‚úÖ Ï†ÑÏ≤¥ CV Í≤ÄÏ¶ù')\n",
                "print(f'  4. ‚úÖ Threshold: {best_threshold:.2f}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DS",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
